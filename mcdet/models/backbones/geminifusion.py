# mcdet/models/backbones/geminifusion.py

import torch
import math
import torch.nn as nn
from torch import Tensor
from collections import OrderedDict
from typing import Dict, List, Union, Optional, Tuple

from mmdet.registry import MODELS
from mmengine.model import BaseModule
from mmengine.runner import CheckpointLoader
import functools
from functools import partial
import warnings
import torch.nn.functional as F


def load_geminifusion_model(model, model_file):
    """Load pretrained model for multimodal GeminiFusion."""
    # load raw state_dict
    if isinstance(model_file, str):
        raw_state_dict = torch.load(model_file, map_location=torch.device('cpu'))
        if 'model' in raw_state_dict.keys():
            raw_state_dict = raw_state_dict['model']
    else:
        raw_state_dict = model_file
    
    state_dict = {}
    for k, v in raw_state_dict.items():
        if k.find('patch_embed') >= 0:
            state_dict[k] = v
        elif k.find('block') >= 0:
            state_dict[k] = v
        elif k.find('norm') >= 0:
            state_dict[k] = v

    msg = model.load_state_dict(state_dict, strict=False)
    print(f"[GeminiFusion] Pretrained model loaded: {msg}")
    del state_dict


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


class GeminiFusionModule(nn.Module):
    """GeminiFusion module for pixel-wise multimodal fusion.
    
    Args:
        dim (int): Input dimension
        num_heads (int): Number of attention heads
        dropout (float): Dropout rate
        layer_noise (float): Layer-adaptive noise factor
    """
    
    def __init__(self, dim, num_heads=8, dropout=0.0, layer_noise=0.1):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.layer_noise = layer_noise
        
        # Intra-modal self-attention
        self.intra_attn = nn.MultiheadAttention(dim, num_heads, dropout=dropout, batch_first=True)
        
        # Inter-modal cross-attention  
        self.inter_attn = nn.MultiheadAttention(dim, num_heads, dropout=dropout, batch_first=True)
        
        # Layer normalization
        self.ln1 = nn.LayerNorm(dim)
        self.ln2 = nn.LayerNorm(dim)
        self.ln3 = nn.LayerNorm(dim)
        
        # MLP
        mlp_hidden_dim = int(dim * 4)
        self.mlp = nn.Sequential(
            nn.Linear(dim, mlp_hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(mlp_hidden_dim, dim),
            nn.Dropout(dropout)
        )
        
        # Fusion gate
        self.fusion_gate = nn.Sequential(
            nn.Linear(dim * 2, dim),
            nn.Sigmoid()
        )
    
    def forward(self, x_rgb, x_modal, H, W):
        """
        Args:
            x_rgb: RGB features [B, N, C]
            x_modal: Other modal features [B, N, C] 
            H, W: Spatial dimensions
        """
        B, N, C = x_rgb.shape
        
        # Intra-modal self-attention for RGB
        x_rgb_norm = self.ln1(x_rgb)
        x_rgb_attn, _ = self.intra_attn(x_rgb_norm, x_rgb_norm, x_rgb_norm)
        x_rgb = x_rgb + x_rgb_attn
        
        # Intra-modal self-attention for other modality
        x_modal_norm = self.ln1(x_modal)
        x_modal_attn, _ = self.intra_attn(x_modal_norm, x_modal_norm, x_modal_norm)
        x_modal = x_modal + x_modal_attn
        
        # Inter-modal cross-attention
        x_rgb_norm2 = self.ln2(x_rgb)
        x_modal_norm2 = self.ln2(x_modal)
        
        # RGB attends to modal
        x_rgb_cross, _ = self.inter_attn(x_rgb_norm2, x_modal_norm2, x_modal_norm2)
        # Modal attends to RGB  
        x_modal_cross, _ = self.inter_attn(x_modal_norm2, x_rgb_norm2, x_rgb_norm2)
        
        # Layer-adaptive noise for fusion control
        if self.training:
            noise_factor = torch.randn(1, device=x_rgb.device) * self.layer_noise
            noise_factor = torch.sigmoid(noise_factor)
        else:
            noise_factor = 0.5
        
        # Pixel-wise fusion with learnable gate
        x_concat = torch.cat([x_rgb_cross, x_modal_cross], dim=-1)
        fusion_weights = self.fusion_gate(x_concat)
        
        x_fused = noise_factor * (fusion_weights * x_rgb_cross + (1 - fusion_weights) * x_modal_cross) + \
                  (1 - noise_factor) * (x_rgb + x_modal) / 2
        
        # MLP
        x_fused_norm = self.ln3(x_fused)
        x_fused = x_fused + self.mlp(x_fused_norm)
        
        return x_fused


class Attention(nn.Module):
    def __init__(self, dim, head, sr_ratio):
        super().__init__()
        self.head = head
        self.sr_ratio = sr_ratio 
        self.scale = (dim // head) ** -0.5
        self.q = nn.Linear(dim, dim)
        self.kv = nn.Linear(dim, dim*2)
        self.proj = nn.Linear(dim, dim)

        if sr_ratio > 1:
            self.sr = nn.Conv2d(dim, dim, sr_ratio, sr_ratio)
            self.norm = nn.LayerNorm(dim)

    def forward(self, x: Tensor, H, W) -> Tensor:
        B, N, C = x.shape
        q = self.q(x).reshape(B, N, self.head, C // self.head).permute(0, 2, 1, 3)

        if self.sr_ratio > 1:
            x = x.permute(0, 2, 1).reshape(B, C, H, W)
            x = self.sr(x).reshape(B, C, -1).permute(0, 2, 1)
            x = self.norm(x)
            
        k, v = self.kv(x).reshape(B, -1, 2, self.head, C // self.head).permute(2, 0, 3, 1, 4)

        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)

        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        return x


class DWConv(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, groups=dim)

    def forward(self, x: Tensor, H, W) -> Tensor:
        B, _, C = x.shape
        x = x.transpose(1, 2).view(B, C, H, W)
        x = self.dwconv(x)
        return x.flatten(2).transpose(1, 2)


class MLP(nn.Module):
    def __init__(self, c1, c2):
        super().__init__()
        self.fc1 = nn.Linear(c1, c2)
        self.dwconv = DWConv(c2)
        self.fc2 = nn.Linear(c2, c1)
        
    def forward(self, x: Tensor, H, W) -> Tensor:
        return self.fc2(F.gelu(self.dwconv(self.fc1(x), H, W)))


class PatchEmbed(nn.Module):
    def __init__(self, c1=3, c2=32, patch_size=7, stride=4, padding=0):
        super().__init__()
        self.proj = nn.Conv2d(c1, c2, patch_size, stride, padding)
        self.norm = nn.LayerNorm(c2)

    def forward(self, x: Tensor) -> Tensor:
        x = self.proj(x)
        _, _, H, W = x.shape
        x = x.flatten(2).transpose(1, 2)
        x = self.norm(x)
        return x, H, W


class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks)."""
    def __init__(self, p: float = None):
        super().__init__()
        self.p = p

    def forward(self, x: Tensor) -> Tensor:
        if self.p == 0. or not self.training:
            return x
        kp = 1 - self.p
        shape = (x.shape[0],) + (1,) * (x.ndim - 1)
        random_tensor = kp + torch.rand(shape, dtype=x.dtype, device=x.device)
        random_tensor.floor_()  # binarize
        return x.div(kp) * random_tensor


class Block(nn.Module):
    def __init__(self, dim, head, sr_ratio=1, dpr=0.):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attn = Attention(dim, head, sr_ratio)
        self.drop_path = DropPath(dpr) if dpr > 0. else nn.Identity()
        self.norm2 = nn.LayerNorm(dim)
        self.mlp = MLP(dim, int(dim*4))

    def forward(self, x: Tensor, H, W) -> Tensor:
        x = x + self.drop_path(self.attn(self.norm1(x), H, W))
        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))
        return x


# GeminiFusion backbone settings
geminifusion_settings = {
    'B0': [[32, 64, 160, 256], [2, 2, 2, 2]],
    'B1': [[64, 128, 320, 512], [2, 2, 2, 2]],
    'B2': [[64, 128, 320, 512], [3, 4, 6, 3]],
    'B3': [[64, 128, 320, 512], [3, 4, 18, 3]],
    'B4': [[64, 128, 320, 512], [3, 8, 27, 3]],
    'B5': [[64, 128, 320, 512], [3, 6, 40, 3]]
}


class GeminiFusion(nn.Module):
    def __init__(self, model_name: str = 'B2', modals: list = ['rgb', 'depth']):
        super().__init__()
        assert model_name in geminifusion_settings.keys(), f"Model name should be in {list(geminifusion_settings.keys())}"
        embed_dims, depths = geminifusion_settings[model_name]
        
        self.modals = modals
        self.num_modals = len(modals)
        drop_path_rate = 0.1
        self.channels = embed_dims

        # Patch embedding layers for each stage
        self.patch_embed1 = PatchEmbed(3, embed_dims[0], 7, 4, 7//2)
        self.patch_embed2 = PatchEmbed(embed_dims[0], embed_dims[1], 3, 2, 3//2)
        self.patch_embed3 = PatchEmbed(embed_dims[1], embed_dims[2], 3, 2, 3//2)
        self.patch_embed4 = PatchEmbed(embed_dims[2], embed_dims[3], 3, 2, 3//2)

        # Additional patch embedding for other modalities
        if self.num_modals > 1:
            self.extra_patch_embed1 = PatchEmbed(3, embed_dims[0], 7, 4, 7//2)
            self.extra_patch_embed2 = PatchEmbed(embed_dims[0], embed_dims[1], 3, 2, 3//2)
            self.extra_patch_embed3 = PatchEmbed(embed_dims[1], embed_dims[2], 3, 2, 3//2)
            self.extra_patch_embed4 = PatchEmbed(embed_dims[2], embed_dims[3], 3, 2, 3//2)

        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]
        
        cur = 0
        # Stage 1
        self.block1 = nn.ModuleList([Block(embed_dims[0], 1, 8, dpr[cur+i]) for i in range(depths[0])])
        self.norm1 = nn.LayerNorm(embed_dims[0])
        if self.num_modals > 1:
            self.extra_block1 = nn.ModuleList([Block(embed_dims[0], 1, 8, dpr[cur+i]) for i in range(depths[0])])
            self.extra_norm1 = nn.LayerNorm(embed_dims[0])
            self.fusion1 = GeminiFusionModule(embed_dims[0], num_heads=1, layer_noise=0.1)

        cur += depths[0]
        # Stage 2
        self.block2 = nn.ModuleList([Block(embed_dims[1], 2, 4, dpr[cur+i]) for i in range(depths[1])])
        self.norm2 = nn.LayerNorm(embed_dims[1])
        if self.num_modals > 1:
            self.extra_block2 = nn.ModuleList([Block(embed_dims[1], 2, 4, dpr[cur+i]) for i in range(depths[1])])
            self.extra_norm2 = nn.LayerNorm(embed_dims[1])
            self.fusion2 = GeminiFusionModule(embed_dims[1], num_heads=2, layer_noise=0.1)

        cur += depths[1]
        # Stage 3
        self.block3 = nn.ModuleList([Block(embed_dims[2], 5, 2, dpr[cur+i]) for i in range(depths[2])])
        self.norm3 = nn.LayerNorm(embed_dims[2])
        if self.num_modals > 1:
            self.extra_block3 = nn.ModuleList([Block(embed_dims[2], 5, 2, dpr[cur+i]) for i in range(depths[2])])
            self.extra_norm3 = nn.LayerNorm(embed_dims[2])
            self.fusion3 = GeminiFusionModule(embed_dims[2], num_heads=5, layer_noise=0.1)

        cur += depths[2]
        # Stage 4
        self.block4 = nn.ModuleList([Block(embed_dims[3], 8, 1, dpr[cur+i]) for i in range(depths[3])])
        self.norm4 = nn.LayerNorm(embed_dims[3])
        if self.num_modals > 1:
            self.extra_block4 = nn.ModuleList([Block(embed_dims[3], 8, 1, dpr[cur+i]) for i in range(depths[3])])
            self.extra_norm4 = nn.LayerNorm(embed_dims[3])
            self.fusion4 = GeminiFusionModule(embed_dims[3], num_heads=8, layer_noise=0.1)

    def forward(self, x: list) -> list:
        if len(x) == 1:  # Single modal
            return self.forward_single_modal(x[0])
        else:  # Multi-modal
            return self.forward_multi_modal(x)
    
    def forward_single_modal(self, x_rgb):
        """Forward pass for single modal (RGB only)"""
        B = x_rgb.shape[0]
        outs = []
        
        # Stage 1
        x_rgb, H, W = self.patch_embed1(x_rgb)
        for blk in self.block1:
            x_rgb = blk(x_rgb, H, W)
        x1_rgb = self.norm1(x_rgb).reshape(B, H, W, -1).permute(0, 3, 1, 2)
        outs.append(x1_rgb)

        # Stage 2
        x_rgb, H, W = self.patch_embed2(x1_rgb)
        for blk in self.block2:
            x_rgb = blk(x_rgb, H, W)
        x2_rgb = self.norm2(x_rgb).reshape(B, H, W, -1).permute(0, 3, 1, 2)
        outs.append(x2_rgb)

        # Stage 3
        x_rgb, H, W = self.patch_embed3(x2_rgb)
        for blk in self.block3:
            x_rgb = blk(x_rgb, H, W)
        x3_rgb = self.norm3(x_rgb).reshape(B, H, W, -1).permute(0, 3, 1, 2)
        outs.append(x3_rgb)

        # Stage 4
        x_rgb, H, W = self.patch_embed4(x3_rgb)
        for blk in self.block4:
            x_rgb = blk(x_rgb, H, W)
        x4_rgb = self.norm4(x_rgb).reshape(B, H, W, -1).permute(0, 3, 1, 2)
        outs.append(x4_rgb)

        return outs
    
    def forward_multi_modal(self, x):
        """Forward pass for multi-modal inputs"""
        x_rgb = x[0]
        x_modal = x[1]  # Assume second modality for simplicity
        B = x_rgb.shape[0]
        outs = []

        # Stage 1
        x_rgb, H, W = self.patch_embed1(x_rgb)
        x_modal, _, _ = self.extra_patch_embed1(x_modal)
        
        for blk in self.block1:
            x_rgb = blk(x_rgb, H, W)
        for blk in self.extra_block1:
            x_modal = blk(x_modal, H, W)
            
        x1_rgb = self.norm1(x_rgb)
        x1_modal = self.extra_norm1(x_modal)
        
        # GeminiFusion
        x1_fused = self.fusion1(x1_rgb, x1_modal, H, W)
        x1_fused = x1_fused.reshape(B, H, W, -1).permute(0, 3, 1, 2)
        outs.append(x1_fused)

        # Stage 2
        x_rgb, H, W = self.patch_embed2(x1_fused)
        x_modal, _, _ = self.extra_patch_embed2(x1_fused)
        
        for blk in self.block2:
            x_rgb = blk(x_rgb, H, W)
        for blk in self.extra_block2:
            x_modal = blk(x_modal, H, W)
            
        x2_rgb = self.norm2(x_rgb)
        x2_modal = self.extra_norm2(x_modal)
        
        x2_fused = self.fusion2(x2_rgb, x2_modal, H, W)
        x2_fused = x2_fused.reshape(B, H, W, -1).permute(0, 3, 1, 2)
        outs.append(x2_fused)

        # Stage 3
        x_rgb, H, W = self.patch_embed3(x2_fused)
        x_modal, _, _ = self.extra_patch_embed3(x2_fused)
        
        for blk in self.block3:
            x_rgb = blk(x_rgb, H, W)
        for blk in self.extra_block3:
            x_modal = blk(x_modal, H, W)
            
        x3_rgb = self.norm3(x_rgb)
        x3_modal = self.extra_norm3(x_modal)
        
        x3_fused = self.fusion3(x3_rgb, x3_modal, H, W)
        x3_fused = x3_fused.reshape(B, H, W, -1).permute(0, 3, 1, 2)
        outs.append(x3_fused)

        # Stage 4
        x_rgb, H, W = self.patch_embed4(x3_fused)
        x_modal, _, _ = self.extra_patch_embed4(x3_fused)
        
        for blk in self.block4:
            x_rgb = blk(x_rgb, H, W)
        for blk in self.extra_block4:
            x_modal = blk(x_modal, H, W)
            
        x4_rgb = self.norm4(x_rgb)
        x4_modal = self.extra_norm4(x_modal)
        
        x4_fused = self.fusion4(x4_rgb, x4_modal, H, W)
        x4_fused = x4_fused.reshape(B, H, W, -1).permute(0, 3, 1, 2)
        outs.append(x4_fused)

        return outs


class GeminiFusionBaseModel(BaseModule):
    """Base model wrapper for GeminiFusion backbone."""
    
    def __init__(self, 
                 backbone: str = 'MiT-B2', 
                 modals: List[str] = ['rgb', 'depth'],
                 init_cfg: Optional[dict] = None) -> None:
        super().__init__(init_cfg=init_cfg)
        
        backbone_name, variant = backbone.split('-')
        self.backbone = GeminiFusion(variant, modals)
        self.modals = modals

    def _init_weights(self, m: nn.Module) -> None:
        """Initialize weights."""
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if m.bias is not None:
                nn.init.zeros_(m.bias)
        elif isinstance(m, nn.Conv2d):
            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
            fan_out // m.groups
            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))
            if m.bias is not None:
                nn.init.zeros_(m.bias)
        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):
            nn.init.ones_(m.weight)
            nn.init.zeros_(m.bias)

    def init_pretrained(self, pretrained: str = None) -> None:
        """Load pretrained weights."""
        if pretrained:
            if len(self.modals) > 1:
                load_geminifusion_model(self.backbone, pretrained)
            else:
                checkpoint = torch.load(pretrained, map_location='cpu')
                if 'state_dict' in checkpoint.keys():
                    checkpoint = checkpoint['state_dict']
                if 'model' in checkpoint.keys():
                    checkpoint = checkpoint['model']
                msg = self.backbone.load_state_dict(checkpoint, strict=False)
                print(f"[GeminiFusion] Single modal pretrained loaded: {msg}")


@MODELS.register_module()
class GeminiFusionBackbone(BaseModule):
    """GeminiFusion backbone for multimodal object detection.
    
    This backbone processes multimodal inputs with efficient pixel-wise fusion
    and outputs multi-scale features for detection.
    
    Args:
        backbone (str): Backbone variant, e.g., 'MiT-B0', 'MiT-B2'
        modals (list): List of modalities to process  
        out_indices (tuple): Output indices for FPN
        frozen_stages (int): Stages to be frozen
        norm_eval (bool): Whether to set norm layers to eval mode
        pretrained (str): Path to pretrained weights
    """
    
    def __init__(self,
                 backbone: str = 'MiT-B2',
                 modals: List[str] = ['rgb', 'depth'],
                 out_indices: Tuple[int] = (0, 1, 2, 3),
                 frozen_stages: int = -1,
                 norm_eval: bool = False,
                 pretrained: Optional[str] = None,
                 init_cfg: Optional[dict] = None):
        
        super().__init__(init_cfg=init_cfg)
        
        self.backbone_name = backbone
        self.modals = modals
        self.out_indices = out_indices
        self.frozen_stages = frozen_stages
        self.norm_eval = norm_eval
        
        # Create GeminiFusion base model
        self.geminifusion_model = GeminiFusionBaseModel(
            backbone=backbone, 
            modals=modals,
            init_cfg=init_cfg
        )
        
        # Determine output channels based on backbone variant
        if 'B0' in backbone:
            self.out_channels = [32, 64, 160, 256]
        elif 'B1' in backbone:
            self.out_channels = [64, 128, 320, 512]
        elif 'B2' in backbone or 'B3' in backbone or 'B4' in backbone or 'B5' in backbone:
            self.out_channels = [64, 128, 320, 512]
        else:
            self.out_channels = [64, 128, 320, 512]  # Default
        
        # Load pretrained weights if provided
        if pretrained:
            self.geminifusion_model.init_pretrained(pretrained)
        
        self._freeze_stages()
    
    def _freeze_stages(self):
        """Freeze stages according to frozen_stages."""
        if self.frozen_stages >= 0:
            # Freeze patch embedding
            if hasattr(self.geminifusion_model.backbone, 'patch_embed1'):
                for param in self.geminifusion_model.backbone.patch_embed1.parameters():
                    param.requires_grad = False
            
            # Freeze stages
            for i in range(self.frozen_stages + 1):
                if hasattr(self.geminifusion_model.backbone, f'block{i+1}'):
                    for param in getattr(self.geminifusion_model.backbone, f'block{i+1}').parameters():
                        param.requires_grad = False
    
    def forward(self, x: List[torch.Tensor]) -> Tuple[torch.Tensor]:
        """Forward pass of GeminiFusion backbone.
        
        Args:
            x: List of multimodal tensors [rgb_tensor, depth_tensor, ...]
               Each tensor has shape (B, C, H, W)
        
        Returns:
            Tuple of feature tensors from different stages
        """
        # GeminiFusion backbone expects list of multimodal inputs
        features = self.geminifusion_model.backbone(x)
        
        # Return features at specified indices
        outs = [features[i] for i in self.out_indices]
        
        return tuple(outs)
    
    def train(self, mode: bool = True):
        """Set train/eval mode."""
        super().train(mode)
        
        if mode and self.norm_eval:
            # Set norm layers to eval mode
            for m in self.modules():
                if isinstance(m, (nn.BatchNorm2d, nn.LayerNorm)):
                    m.eval()
    
    def init_weights(self):
        """Initialize weights of the backbone."""
        if self.init_cfg is None:
            # Apply custom weight initialization
            self.geminifusion_model.apply(self.geminifusion_model._init_weights)
        else:
            super().init_weights()